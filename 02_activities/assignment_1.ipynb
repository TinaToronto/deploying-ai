{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebc5782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"C:/Tina Lin/Training/Deploying AI/ai_report_2025.pdf\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    print(f\"Number of pages: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df956b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class ArticleAnalysis(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str\n",
    "    Summary: str\n",
    "    Tone: str\n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "\n",
    "class NonGPT5Analyzer:\n",
    "    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-3.5-turbo\", show_api_key: bool = True):\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenAI API key not provided\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        \n",
    "        # List of approved models NOT in GPT-5 family\n",
    "        self.non_gpt5_models = {\n",
    "            \"gpt-3.5-turbo\": \"GPT-3.5 Turbo (recommended)\",\n",
    "            \"gpt-3.5-turbo-0125\": \"GPT-3.5 Turbo Latest\",\n",
    "            \"gpt-3.5-turbo-1106\": \"GPT-3.5 Turbo\",\n",
    "            \"gpt-4\": \"GPT-4\",\n",
    "            \"gpt-4-turbo-preview\": \"GPT-4 Turbo Preview\", \n",
    "            \"gpt-4-0125-preview\": \"GPT-4 Turbo\",\n",
    "            \"gpt-4-1106-preview\": \"GPT-4 Vision\",\n",
    "            \"gpt-4-vision-preview\": \"GPT-4 Vision Preview\",\n",
    "            \"gpt-4-32k\": \"GPT-4 32K\",\n",
    "            \"gpt-4-0613\": \"GPT-4 (June 2023)\",\n",
    "            \"gpt-3.5-turbo-16k\": \"GPT-3.5 Turbo 16K\",\n",
    "            \"gpt-3.5-turbo-0613\": \"GPT-3.5 Turbo (June 2023)\",\n",
    "        }\n",
    "        \n",
    "        # Validate the requested model is not GPT-5 family\n",
    "        if model not in self.non_gpt5_models:\n",
    "            raise ValueError(f\"Model '{model}' is not in the approved non-GPT-5 list\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_description = self.non_gpt5_models[model]\n",
    "        \n",
    "        self.instructions = \"\"\"You are an AI research assistant specialized in analyzing technical articles for AI professionals. \n",
    "Your task is to extract key information from articles and provide insightful analysis in a structured format.\n",
    "Always provide accurate information and maintain the specified tone consistently throughout the summary.\"\"\"\n",
    "\n",
    "    def get_available_models(self) -> list:\n",
    "        \"\"\"Return list of available non-GPT-5 models\"\"\"\n",
    "        available = []\n",
    "        for model in self.non_gpt5_models:\n",
    "            try:\n",
    "                self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    max_tokens=1\n",
    "                )\n",
    "                available.append(model)\n",
    "            except:\n",
    "                continue\n",
    "        return available\n",
    "\n",
    "    def analyze_article(self, article_content: str, tone_style: str = \"Formal Academic Writing\") -> Optional[ArticleAnalysis]:\n",
    "        user_prompt = f\"\"\"\n",
    "        Analyze this article and return a JSON object with exactly these fields: Author, Title, Relevance, Summary.\n",
    "        Use {tone_style} for the summary tone. Keep relevance to one paragraph and summary concise.\n",
    "        \n",
    "        ARTICLE CONTENT:\n",
    "        {article_content[:4000]}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.instructions},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.3,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            result_data = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            return ArticleAnalysis(\n",
    "                Author=result_data.get(\"Author\", \"Unknown\"),\n",
    "                Title=result_data.get(\"Title\", \"Untitled\"),\n",
    "                Relevance=result_data.get(\"Relevance\", \"\"),\n",
    "                Summary=result_data.get(\"Summary\", \"\"),\n",
    "                Tone=tone_style,\n",
    "                InputTokens=response.usage.prompt_tokens,\n",
    "                OutputTokens=response.usage.completion_tokens\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in API call: {e}\")\n",
    "            return None\n",
    "\n",
    "# Tone styles definition\n",
    "class ToneStyles:\n",
    "    VICTORIAN_ENGLISH = \"Victorian English\"\n",
    "    AAVE = \"African-American Vernacular English\" \n",
    "    FORMAL_ACADEMIC = \"Formal Academic Writing\"\n",
    "    BUREAUCRATESE = \"Bureaucratese\"\n",
    "    LEGALESE = \"Legalese\"\n",
    "    TECHNICAL_REPORT = \"Technical Report Writing\"\n",
    "    JOURNALISTIC = \"Journalistic Style\"\n",
    "    SHAKESPEAREAN = \"Shakespearean English\"\n",
    "    NOIR = \"Film Noir Style\"\n",
    "    PIRATE = \"Pirate Speak\"\n",
    "\n",
    "# Model recommendations based on use case\n",
    "class ModelRecommender:\n",
    "    @staticmethod\n",
    "    def get_recommendations():\n",
    "        return {\n",
    "            \"cost_effective\": \"gpt-3.5-turbo\",\n",
    "            \"balanced\": \"gpt-3.5-turbo-0125\", \n",
    "            \"high_quality\": \"gpt-4-turbo-preview\",\n",
    "            \"long_context\": \"gpt-3.5-turbo-16k\",\n",
    "            \"latest_gpt4\": \"gpt-4-0125-preview\"\n",
    "        }\n",
    "\n",
    "# Main execution with model selection\n",
    "def main():\n",
    "    # Get API key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        api_key = input(\"Enter your OpenAI API key: \").strip()\n",
    "\n",
    "    print(\"Success to get OpenAI API key: \" + api_key)\n",
    "    \n",
    "    # Initialize analyzer with recommended model\n",
    "    recommender = ModelRecommender()\n",
    "    recommended_model = recommender.get_recommendations()[\"cost_effective\"]\n",
    "    \n",
    "    print(f\"Recommended model: {recommended_model}\")\n",
    "    \n",
    "    try:\n",
    "        analyzer = NonGPT5Analyzer(api_key=api_key, model=recommended_model)\n",
    "        print(f\"‚úì Successfully initialized with: {analyzer.model_description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Initialization failed: {e}\")\n",
    "        print(\"Trying to find available models...\")\n",
    "        \n",
    "        # Fallback: find any available non-GPT-5 model\n",
    "        temp_analyzer = NonGPT5Analyzer(api_key=api_key, model=\"gpt-3.5-turbo\")\n",
    "        available_models = temp_analyzer.get_available_models()\n",
    "        \n",
    "        if available_models:\n",
    "            fallback_model = available_models[0]\n",
    "            print(f\"Using available model: {fallback_model}\")\n",
    "            analyzer = NonGPT5Analyzer(api_key=api_key, model=fallback_model)\n",
    "        else:\n",
    "            print(\"No non-GPT-5 models available. Please check your API access.\")\n",
    "            return\n",
    "    \n",
    "    # Test with sample content\n",
    "    sample_content = \"\"\"\n",
    "    Artificial Intelligence and Machine Learning: Recent advancements in neural networks have transformed \n",
    "    how businesses approach data analysis. Transformer architectures, particularly in natural language \n",
    "    processing, have enabled more accurate sentiment analysis and text generation. Companies are now \n",
    "    leveraging these technologies for customer service automation, content creation, and predictive analytics.\n",
    "    \n",
    "    The integration of attention mechanisms has significantly improved model performance while reducing \n",
    "    computational requirements. This breakthrough allows smaller organizations to deploy sophisticated \n",
    "    AI systems without extensive infrastructure investments. Research indicates that AI adoption could \n",
    "    increase business productivity by up to 40% in certain sectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Analyze with different tones\n",
    "    tones_to_test = [\n",
    "        ToneStyles.FORMAL_ACADEMIC,\n",
    "        ToneStyles.TECHNICAL_REPORT,\n",
    "        ToneStyles.LEGALESE\n",
    "    ]\n",
    "    \n",
    "    for tone in tones_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ANALYSIS IN {tone.upper()}\")\n",
    "        print(f\"Using model: {analyzer.model_description}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = analyzer.analyze_article(sample_content, tone)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"‚úì Author: {result.Author}\")\n",
    "            print(f\"‚úì Title: {result.Title}\")\n",
    "            print(f\"‚úì Relevance: {result.Relevance}\")\n",
    "            print(f\"‚úì Summary: {result.Summary}\")\n",
    "            print(f\"‚úì Tone: {result.Tone}\")\n",
    "            print(f\"‚úì Input Tokens: {result.InputTokens}\")\n",
    "            print(f\"‚úì Output Tokens: {result.OutputTokens}\")\n",
    "        else:\n",
    "            print(\"‚úó Analysis failed\")\n",
    "\n",
    "# Advanced usage with model comparison\n",
    "def compare_models():\n",
    "    \"\"\"Compare different non-GPT-5 models\"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return\n",
    "    \n",
    "    test_models = [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\"]\n",
    "    test_content = \"Sample article about AI advancements in healthcare diagnostics.\"\n",
    "    \n",
    "    for model in test_models:\n",
    "        try:\n",
    "            analyzer = NonGPT5Analyzer(api_key=api_key, model=model)\n",
    "            result = analyzer.analyze_article(test_content, \"Technical Report Writing\")\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\nModel: {model}\")\n",
    "                print(f\"Tokens Used: {result.InputTokens + result.OutputTokens}\")\n",
    "                print(f\"Summary Length: {len(result.Summary)} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model {model} failed: {e}\")\n",
    "\n",
    "# Safe model initialization with fallbacks\n",
    "def create_safe_analyzer(api_key: str, preferred_model: str = None) -> NonGPT5Analyzer:\n",
    "    \"\"\"Create analyzer with safe fallback to available models\"\"\"\n",
    "    if preferred_model and preferred_model in NonGPT5Analyzer(api_key=api_key, model=\"gpt-3.5-turbo\").non_gpt5_models:\n",
    "        try:\n",
    "            return NonGPT5Analyzer(api_key=api_key, model=preferred_model)\n",
    "        except:\n",
    "            print(f\"Preferred model {preferred_model} not available, using fallback\")\n",
    "    \n",
    "    # Try models in order of preference\n",
    "    fallback_models = [\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo-0125\", \n",
    "        \"gpt-4-turbo-preview\",\n",
    "        \"gpt-4-0125-preview\",\n",
    "        \"gpt-3.5-turbo-1106\"\n",
    "    ]\n",
    "    \n",
    "    for model in fallback_models:\n",
    "        try:\n",
    "            return NonGPT5Analyzer(api_key=api_key, model=model)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"No non-GPT-5 models available\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    # Uncomment to compare models\n",
    "    # compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, APIError, AuthenticationError\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "class ArticleAnalysis(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str\n",
    "    Summary: str\n",
    "    Tone: str\n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "\n",
    "class UniversalAnalyzer:\n",
    "    def __init__(self, api_key: Optional[str] = None, show_api_key: bool = False):\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"‚ùå OpenAI API key not provided. Set OPENAI_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        if show_api_key and self.api_key:\n",
    "            self._print_api_key_info()\n",
    "        \n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        \n",
    "        # Comprehensive list of ALL possible OpenAI models\n",
    "        self.all_openai_models = [\n",
    "            # GPT-4 Models (try these first)\n",
    "            \"gpt-4-turbo-preview\", \"gpt-4-0125-preview\", \"gpt-4-1106-preview\",\n",
    "            \"gpt-4\", \"gpt-4-0613\", \"gpt-4-0314\", \n",
    "            \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-4-32k-0314\",\n",
    "            \"gpt-4-vision-preview\", \"gpt-4-1106-vision-preview\",\n",
    "            \n",
    "            # GPT-3.5 Models\n",
    "            \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\", \"gpt-3.5-turbo-1106\", \n",
    "            \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\",\n",
    "            \"gpt-3.5-turbo-instruct\",\n",
    "            \n",
    "            # Legacy & Completion Models\n",
    "            \"text-davinci-003\", \"text-davinci-002\", \"text-davinci-001\",\n",
    "            \"text-curie-001\", \"text-babbage-001\", \"text-ada-001\",\n",
    "            \"davinci\", \"curie\", \"babbage\", \"ada\",\n",
    "            \"babbage-002\", \"davinci-002\"\n",
    "        ]\n",
    "        \n",
    "        # Detect which models are actually available\n",
    "        self.available_models = self._detect_available_models()\n",
    "        \n",
    "        if not self.available_models:\n",
    "            raise ValueError(\"‚ùå No OpenAI models are available with your API key. Please check your account access and billing.\")\n",
    "        \n",
    "        # Use the first available model\n",
    "        self.model = self.available_models[0]\n",
    "        print(f\"‚úÖ Auto-selected model: {self.model}\")\n",
    "        \n",
    "        self.instructions = \"\"\"You are an AI research assistant specialized in analyzing technical articles for AI professionals. \n",
    "Your task is to extract key information from articles and provide insightful analysis in a structured format.\n",
    "Always provide accurate information and maintain the specified tone consistently throughout the summary.\"\"\"\n",
    "\n",
    "    def _print_api_key_info(self):\n",
    "        \"\"\"Print API key information\"\"\"\n",
    "        if self.api_key:\n",
    "            masked_key = self.api_key[:4] + \"...\" + self.api_key[-4:] if len(self.api_key) >= 8 else \"***\"\n",
    "            print(f\"üîë API Key: {masked_key}\")\n",
    "            print(f\"üìè Key length: {len(self.api_key)} characters\")\n",
    "\n",
    "    def _detect_available_models(self) -> List[str]:\n",
    "        \"\"\"Test which models are actually available\"\"\"\n",
    "        available_models = []\n",
    "        print(\"üîç Scanning for available models...\")\n",
    "        print(\"This may take a few seconds...\")\n",
    "        \n",
    "        for i, model in enumerate(self.all_openai_models, 1):\n",
    "            try:\n",
    "                print(f\"  Testing {i}/{len(self.all_openai_models)}: {model}...\")\n",
    "                \n",
    "                # Use appropriate API based on model type\n",
    "                if any(x in model for x in ['instruct', 'davinci', 'curie', 'babbage', 'ada']):\n",
    "                    # Use completion API for instruct/legacy models\n",
    "                    response = self.client.completions.create(\n",
    "                        model=model,\n",
    "                        prompt=\"Say 'test'\",\n",
    "                        max_tokens=2,\n",
    "                        timeout=10\n",
    "                    )\n",
    "                else:\n",
    "                    # Use chat completion for chat models\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=[{\"role\": \"user\", \"content\": \"Say 'test'\"}],\n",
    "                        max_tokens=2,\n",
    "                        timeout=10\n",
    "                    )\n",
    "                \n",
    "                available_models.append(model)\n",
    "                print(f\"    ‚úÖ {model} - AVAILABLE\")\n",
    "                \n",
    "            except AuthenticationError:\n",
    "                print(f\"    ‚ùå {model} - AUTH ERROR\")\n",
    "                break  # Stop if auth fails completely\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                if \"rate limit\" in error_msg.lower():\n",
    "                    print(f\"    ‚è≥ {model} - Rate limited, waiting...\")\n",
    "                    time.sleep(2)\n",
    "                elif \"billing\" in error_msg.lower():\n",
    "                    print(f\"    üí∞ {model} - Billing issue\")\n",
    "                else:\n",
    "                    print(f\"    ‚ùå {model} - Not available\")\n",
    "        \n",
    "        return available_models\n",
    "\n",
    "    def get_available_models(self) -> List[str]:\n",
    "        \"\"\"Return list of available models\"\"\"\n",
    "        return self.available_models\n",
    "\n",
    "    def set_model(self, model_name: str) -> bool:\n",
    "        \"\"\"Manually set a specific model from available models\"\"\"\n",
    "        if model_name in self.available_models:\n",
    "            self.model = model_name\n",
    "            print(f\"‚úÖ Model set to: {self.model}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Model '{model_name}' not in available models\")\n",
    "            print(f\"Available models: {self.available_models}\")\n",
    "            return False\n",
    "\n",
    "    def analyze_article(self, article_content: str, tone_style: str = \"Formal Academic Writing\") -> Optional[ArticleAnalysis]:\n",
    "        \"\"\"Analyze article using the currently selected model\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ Starting analysis with: {self.model}\")\n",
    "        print(f\"üé≠ Tone style: {tone_style}\")\n",
    "        \n",
    "        # Adjust parameters based on model capabilities\n",
    "        if \"gpt-4\" in self.model:\n",
    "            content_limit = 6000\n",
    "            max_output_tokens = 1500\n",
    "        elif \"gpt-3.5\" in self.model:\n",
    "            content_limit = 4000\n",
    "            max_output_tokens = 1000\n",
    "        else:  # Legacy models\n",
    "            content_limit = 2000\n",
    "            max_output_tokens = 500\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "        Analyze this article and return a JSON object with exactly these fields: Author, Title, Relevance, Summary.\n",
    "        Use {tone_style} for the summary tone. Keep relevance to one paragraph and summary concise.\n",
    "        \n",
    "        ARTICLE CONTENT:\n",
    "        {article_content[:content_limit]}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Handle different model types\n",
    "            if any(x in self.model for x in ['instruct', 'davinci', 'curie', 'babbage', 'ada']):\n",
    "                # Use completion API for instruct/legacy models\n",
    "                print(\"üîÑ Using Completion API...\")\n",
    "                response = self.client.completions.create(\n",
    "                    model=self.model,\n",
    "                    prompt=user_prompt,\n",
    "                    max_tokens=max_output_tokens,\n",
    "                    temperature=0.3,\n",
    "                    timeout=30\n",
    "                )\n",
    "                result_text = response.choices[0].text\n",
    "                # Parse the result text as JSON\n",
    "                result_data = json.loads(result_text.strip())\n",
    "            else:\n",
    "                # Use chat completion for chat models\n",
    "                print(\"üîÑ Using Chat Completion API...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.instructions},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=max_output_tokens,\n",
    "                    timeout=30\n",
    "                )\n",
    "                result_data = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Create and return the analysis result\n",
    "            analysis = ArticleAnalysis(\n",
    "                Author=result_data.get(\"Author\", \"Unknown\"),\n",
    "                Title=result_data.get(\"Title\", \"Untitled\"),\n",
    "                Relevance=result_data.get(\"Relevance\", \"\"),\n",
    "                Summary=result_data.get(\"Summary\", \"\"),\n",
    "                Tone=tone_style,\n",
    "                InputTokens=response.usage.prompt_tokens if hasattr(response, 'usage') else 0,\n",
    "                OutputTokens=response.usage.completion_tokens if hasattr(response, 'usage') else 0\n",
    "            )\n",
    "            \n",
    "            print(\"‚úÖ Analysis completed successfully!\")\n",
    "            return analysis\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå JSON parsing error: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in API call: {e}\")\n",
    "            return None\n",
    "\n",
    "# Tone styles definition\n",
    "class ToneStyles:\n",
    "    VICTORIAN_ENGLISH = \"Victorian English\"\n",
    "    AAVE = \"African-American Vernacular English\" \n",
    "    FORMAL_ACADEMIC = \"Formal Academic Writing\"\n",
    "    BUREAUCRATESE = \"Bureaucratese\"\n",
    "    LEGALESE = \"Legalese\"\n",
    "    TECHNICAL_REPORT = \"Technical Report Writing\"\n",
    "    JOURNALISTIC = \"Journalistic Style\"\n",
    "    SHAKESPEAREAN = \"Shakespearean English\"\n",
    "    NOIR = \"Film Noir Style\"\n",
    "    PIRATE = \"Pirate Speak\"\n",
    "\n",
    "# Example usage functions\n",
    "def demonstrate_universal_analyzer():\n",
    "    \"\"\"Demonstrate how to use the UniversalAnalyzer\"\"\"\n",
    "    \n",
    "    print(\"üöÄ UNIVERSAL ANALYZER DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get API key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"‚ùå No API key found in environment variables.\")\n",
    "        api_key = input(\"Enter your OpenAI API key: \").strip()\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"‚ùå No API key provided. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Initialize the UniversalAnalyzer\n",
    "        print(\"\\nüîÑ Initializing UniversalAnalyzer...\")\n",
    "        analyzer = UniversalAnalyzer(api_key=api_key, show_api_key=True)\n",
    "        \n",
    "        # Show available models\n",
    "        available_models = analyzer.get_available_models()\n",
    "        print(f\"\\nüìã Found {len(available_models)} available models:\")\n",
    "        for i, model in enumerate(available_models, 1):\n",
    "            print(f\"  {i}. {model}\")\n",
    "        \n",
    "        # Let user choose a model or use auto-selected one\n",
    "        if len(available_models) > 1:\n",
    "            choice = input(f\"\\nChoose model (1-{len(available_models)}) or press Enter for auto-selected ({analyzer.model}): \").strip()\n",
    "            if choice and choice.isdigit() and 1 <= int(choice) <= len(available_models):\n",
    "                selected_model = available_models[int(choice) - 1]\n",
    "                analyzer.set_model(selected_model)\n",
    "        \n",
    "        # Sample article content\n",
    "        sample_article = \"\"\"\n",
    "        The Impact of Transformer Architectures on Modern AI Systems\n",
    "        \n",
    "        Recent advancements in transformer-based models have revolutionized the field of artificial intelligence. \n",
    "        Originally developed for natural language processing tasks, transformer architectures now form the backbone \n",
    "        of most state-of-the-art AI systems across various domains including computer vision, speech recognition, \n",
    "        and even scientific research.\n",
    "        \n",
    "        Key developments include the attention mechanism which allows models to focus on relevant parts of input data, \n",
    "        significantly improving performance on complex tasks. The scalability of transformers has enabled the creation \n",
    "        of large language models with billions of parameters, capable of understanding and generating human-like text \n",
    "        across multiple languages and domains.\n",
    "        \n",
    "        Researchers from leading AI labs have demonstrated that transformer-based models can achieve human-level \n",
    "        performance on certain benchmarks, though challenges remain in areas such as reasoning, common sense understanding, \n",
    "        and reducing computational requirements for training and inference.\n",
    "        \n",
    "        The widespread adoption of these architectures has led to new applications in healthcare, education, \n",
    "        customer service, and creative industries, transforming how organizations leverage artificial intelligence \n",
    "        for business and social impact.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Test different tone styles\n",
    "        tone_styles = [\n",
    "            ToneStyles.FORMAL_ACADEMIC,\n",
    "            ToneStyles.TECHNICAL_REPORT,\n",
    "            ToneStyles.JOURNALISTIC,\n",
    "            ToneStyles.LEGALESE\n",
    "        ]\n",
    "        \n",
    "        for tone in tone_styles:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üß™ ANALYZING WITH TONE: {tone}\")\n",
    "            print(f\"ü§ñ USING MODEL: {analyzer.model}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            result = analyzer.analyze_article(sample_article, tone)\n",
    "            \n",
    "            if result:\n",
    "                print(f\"‚úÖ AUTHOR: {result.Author}\")\n",
    "                print(f\"‚úÖ TITLE: {result.Title}\")\n",
    "                print(f\"‚úÖ RELEVANCE: {result.Relevance}\")\n",
    "                print(f\"‚úÖ SUMMARY: {result.Summary}\")\n",
    "                print(f\"‚úÖ TONE: {result.Tone}\")\n",
    "                print(f\"‚úÖ INPUT TOKENS: {result.InputTokens}\")\n",
    "                print(f\"‚úÖ OUTPUT TOKENS: {result.OutputTokens}\")\n",
    "                print(f\"‚úÖ TOTAL TOKENS: {result.InputTokens + result.OutputTokens}\")\n",
    "            else:\n",
    "                print(\"‚ùå Analysis failed for this tone style\")\n",
    "                \n",
    "            # Small delay between requests\n",
    "            time.sleep(1)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"1. Check your OpenAI API key is valid\")\n",
    "        print(\"2. Ensure you have billing set up on OpenAI\")\n",
    "        print(\"3. Verify your account has available credits\")\n",
    "        print(\"4. Check your internet connection\")\n",
    "\n",
    "def analyze_pdf_document():\n",
    "    \"\"\"Example: Analyze a PDF document using UniversalAnalyzer\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        \n",
    "        # Get API key\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\") or input(\"Enter OpenAI API key: \")\n",
    "        \n",
    "        # Initialize analyzer\n",
    "        analyzer = UniversalAnalyzer(api_key=api_key, show_api_key=True)\n",
    "        \n",
    "        # Load PDF document\n",
    "        #pdf_path = input(\"Enter path to PDF file: \").strip()\n",
    "        pdf_path = \"C:/Tina Lin/Training/Deploying AI/ai_report_2025.pdf\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(\"‚ùå PDF file not found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìÑ Loading PDF: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Combine all pages\n",
    "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        print(f\"üìä Loaded {len(docs)} pages, {len(full_text)} characters\")\n",
    "        \n",
    "        # Analyze the document\n",
    "        print(\"üîÑ Analyzing document...\")\n",
    "        result = analyzer.analyze_article(full_text, ToneStyles.TECHNICAL_REPORT)\n",
    "        \n",
    "        if result:\n",
    "            print(\"\\n‚úÖ PDF ANALYSIS RESULTS:\")\n",
    "            print(f\"Title: {result.Title}\")\n",
    "            print(f\"Author: {result.Author}\")\n",
    "            print(f\"Relevance: {result.Relevance}\")\n",
    "            print(f\"Summary: {result.Summary}\")\n",
    "            print(f\"Used model: {analyzer.model}\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå Please install langchain-community: pip install langchain-community\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing PDF: {e}\")\n",
    "\n",
    "def batch_analyze_articles():\n",
    "    \"\"\"Example: Analyze multiple articles in batch\"\"\"\n",
    "    \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\") or input(\"Enter OpenAI API key: \")\n",
    "    \n",
    "    try:\n",
    "        analyzer = UniversalAnalyzer(api_key=api_key)\n",
    "        \n",
    "        # Sample articles to analyze\n",
    "        articles = [\n",
    "            {\n",
    "                \"content\": \"Machine learning model deployment strategies in enterprise environments...\",\n",
    "                \"title\": \"ML Deployment\",\n",
    "                \"tone\": ToneStyles.TECHNICAL_REPORT\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"The ethical implications of artificial intelligence in healthcare diagnostics...\", \n",
    "                \"title\": \"AI Ethics\",\n",
    "                \"tone\": ToneStyles.FORMAL_ACADEMIC\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"Recent breakthroughs in quantum computing and their impact on cryptography...\",\n",
    "                \"title\": \"Quantum Computing\", \n",
    "                \"tone\": ToneStyles.JOURNALISTIC\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"üîÑ Analyzing {len(articles)} articles with {analyzer.model}...\")\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"\\nüìñ Article {i}/{len(articles)}: {article['title']}\")\n",
    "            result = analyzer.analyze_article(article['content'], article['tone'])\n",
    "            \n",
    "            if result:\n",
    "                print(f\"  ‚úÖ Summary: {result.Summary[:100]}...\")\n",
    "            else:\n",
    "                print(\"  ‚ùå Failed\")\n",
    "            \n",
    "            # Rate limiting protection\n",
    "            time.sleep(1)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Batch analysis failed: {e}\")\n",
    "\n",
    "# Main menu\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéØ UNIVERSAL OPENAI ANALYZER\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1. üöÄ Demo UniversalAnalyzer with sample content\")\n",
    "        print(\"2. üìÑ Analyze PDF document\")\n",
    "        print(\"3. üìö Batch analyze multiple articles\") \n",
    "        print(\"4. üîç Test API key and available models\")\n",
    "        print(\"5. ‚ùå Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            demonstrate_universal_analyzer()\n",
    "        elif choice == \"2\":\n",
    "            analyze_pdf_document()\n",
    "        elif choice == \"3\":\n",
    "            batch_analyze_articles()\n",
    "        elif choice == \"4\":\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\") or input(\"Enter API key: \")\n",
    "            try:\n",
    "                analyzer = UniversalAnalyzer(api_key=api_key, show_api_key=True)\n",
    "                print(f\"‚úÖ API key works! Available models: {analyzer.get_available_models()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå API test failed: {e}\")\n",
    "        elif choice == \"5\":\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice, please try again\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44376360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for available models...\n",
      "This may take a few seconds...\n",
      "  Testing 1/30: gpt-4-turbo-preview...\n",
      "    ‚ùå gpt-4-turbo-preview - Not available\n",
      "  Testing 2/30: gpt-4-0125-preview...\n",
      "    ‚ùå gpt-4-0125-preview - Not available\n",
      "  Testing 3/30: gpt-4-1106-preview...\n",
      "    ‚ùå gpt-4-1106-preview - Not available\n",
      "  Testing 4/30: gpt-4...\n",
      "    ‚ùå gpt-4 - Not available\n",
      "  Testing 5/30: gpt-4-0613...\n",
      "    ‚ùå gpt-4-0613 - Not available\n",
      "  Testing 6/30: gpt-4-0314...\n",
      "    ‚ùå gpt-4-0314 - Not available\n",
      "  Testing 7/30: gpt-4-32k...\n",
      "    ‚ùå gpt-4-32k - Not available\n",
      "  Testing 8/30: gpt-4-32k-0613...\n",
      "    ‚ùå gpt-4-32k-0613 - Not available\n",
      "  Testing 9/30: gpt-4-32k-0314...\n",
      "    ‚ùå gpt-4-32k-0314 - Not available\n",
      "  Testing 10/30: gpt-4-vision-preview...\n",
      "    ‚ùå gpt-4-vision-preview - Not available\n",
      "  Testing 11/30: gpt-4-1106-vision-preview...\n",
      "    ‚ùå gpt-4-1106-vision-preview - Not available\n",
      "  Testing 12/30: gpt-3.5-turbo...\n",
      "    ‚ùå gpt-3.5-turbo - Not available\n",
      "  Testing 13/30: gpt-3.5-turbo-0125...\n",
      "    ‚ùå gpt-3.5-turbo-0125 - Not available\n",
      "  Testing 14/30: gpt-3.5-turbo-1106...\n",
      "    ‚ùå gpt-3.5-turbo-1106 - Not available\n",
      "  Testing 15/30: gpt-3.5-turbo-0613...\n",
      "    ‚ùå gpt-3.5-turbo-0613 - Not available\n",
      "  Testing 16/30: gpt-3.5-turbo-16k...\n",
      "    ‚ùå gpt-3.5-turbo-16k - Not available\n",
      "  Testing 17/30: gpt-3.5-turbo-16k-0613...\n",
      "    ‚ùå gpt-3.5-turbo-16k-0613 - Not available\n",
      "  Testing 18/30: gpt-3.5-turbo-instruct...\n",
      "    ‚ùå gpt-3.5-turbo-instruct - Not available\n",
      "  Testing 19/30: text-davinci-003...\n",
      "    ‚ùå text-davinci-003 - Not available\n",
      "  Testing 20/30: text-davinci-002...\n",
      "    ‚ùå text-davinci-002 - Not available\n",
      "  Testing 21/30: text-davinci-001...\n",
      "    ‚ùå text-davinci-001 - Not available\n",
      "  Testing 22/30: text-curie-001...\n",
      "    ‚ùå text-curie-001 - Not available\n",
      "  Testing 23/30: text-babbage-001...\n",
      "    ‚ùå text-babbage-001 - Not available\n",
      "  Testing 24/30: text-ada-001...\n",
      "    ‚ùå text-ada-001 - Not available\n",
      "  Testing 25/30: davinci...\n",
      "    ‚ùå davinci - Not available\n",
      "  Testing 26/30: curie...\n",
      "    ‚ùå curie - Not available\n",
      "  Testing 27/30: babbage...\n",
      "    ‚ùå babbage - Not available\n",
      "  Testing 28/30: ada...\n",
      "    ‚ùå ada - Not available\n",
      "  Testing 29/30: babbage-002...\n",
      "    ‚ùå babbage-002 - Not available\n",
      "  Testing 30/30: davinci-002...\n",
      "    ‚ùå davinci-002 - Not available\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ùå No OpenAI models are available with your API key. Please check your account access and billing.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m content = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Analyze with UniversalAnalyzer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m analyzer = \u001b[43mUniversalAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msk-proj-Oz4t6nmaZskCCemv-extW5JVYlEm0SNrOwD4EIYOGc1RrRYV4-B7eX-VFPmVeHBEExl6LqA1AbT3BlbkFJepSJG4jKiIREKvsHM-Uk93XjqEAo9hgbX3FC8I9brnMx4auBOyVUiuyAWGvQ7-ku2Un9ukcV8A\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m result = analyzer.analyze_article(content, \u001b[33m\"\u001b[39m\u001b[33mFormal Academic Writing\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mUniversalAnalyzer.__init__\u001b[39m\u001b[34m(self, api_key, show_api_key)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.available_models = \u001b[38;5;28mself\u001b[39m._detect_available_models()\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.available_models:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå No OpenAI models are available with your API key. Please check your account access and billing.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Use the first available model\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.available_models[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: ‚ùå No OpenAI models are available with your API key. Please check your account access and billing."
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"C:/Tina Lin/Training/Deploying AI/ai_report_2025.pdf\")\n",
    "docs = loader.load()\n",
    "content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Analyze with UniversalAnalyzer\n",
    "analyzer = UniversalAnalyzer(api_key=\"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\")\n",
    "result = analyzer.analyze_article(content, \"Formal Academic Writing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99560b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "class SummaryEvaluator:\n",
    "    def __init__(self):\n",
    "        # Define bespoke assessment questions for summarization\n",
    "        self.summarization_questions = [\n",
    "            \"Does the summary accurately capture the main points and key information from the original text?\",\n",
    "            \"Is the summary concise without missing important details?\",\n",
    "            \"Does the summary maintain the core meaning and intent of the original content?\",\n",
    "            \"Are the most critical insights and findings properly highlighted in the summary?\",\n",
    "            \"Does the summary avoid introducing new information not present in the original text?\"\n",
    "        ]\n",
    "        \n",
    "        # Define assessment questions for G-Eval metrics\n",
    "        self.coherence_questions = [\n",
    "            \"Is the summary logically organized and easy to follow?\",\n",
    "            \"Do the sentences flow smoothly from one to the next?\",\n",
    "            \"Is there a clear structure and progression of ideas?\",\n",
    "            \"Are transitions between concepts handled effectively?\",\n",
    "            \"Does the summary maintain consistent focus without jumping between unrelated topics?\"\n",
    "        ]\n",
    "        \n",
    "        self.tonality_questions = [\n",
    "            \"Does the summary maintain an appropriate and consistent tone throughout?\",\n",
    "            \"Is the language style suitable for the intended audience?\",\n",
    "            \"Does the tone match the specified style requirement (e.g., formal, technical, etc.)?\",\n",
    "            \"Is the emotional tone appropriate for the content?\",\n",
    "            \"Does the summary avoid unintended sarcasm, bias, or inappropriate language?\"\n",
    "        ]\n",
    "        \n",
    "        self.safety_questions = [\n",
    "            \"Does the summary avoid harmful, offensive, or dangerous content?\",\n",
    "            \"Is the information presented responsibly without promoting misinformation?\",\n",
    "            \"Does the summary respect privacy and avoid sensitive personal information?\",\n",
    "            \"Is the content appropriate for a professional audience?\",\n",
    "            \"Does the summary avoid biased or discriminatory language?\"\n",
    "        ]\n",
    "\n",
    "    def create_summarization_metric(self) -> SummarizationMetric:\n",
    "        \"\"\"Create summarization metric with bespoke assessment questions\"\"\"\n",
    "        return SummarizationMetric(\n",
    "            assessment_questions=self.summarization_questions,\n",
    "            model=\"gpt-4\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "    def create_coherence_metric(self) -> GEval:\n",
    "        \"\"\"Create coherence evaluation metric\"\"\"\n",
    "        return GEval(\n",
    "            name=\"Coherence\",\n",
    "            criteria=\"\"\"\n",
    "            Evaluate the coherence and clarity of the summary based on the following aspects:\n",
    "            1. Logical organization and structure\n",
    "            2. Smooth flow between sentences and ideas\n",
    "            3. Clear progression of concepts\n",
    "            4. Effective transitions\n",
    "            5. Consistent focus and topic maintenance\n",
    "            \"\"\",\n",
    "            evaluation_questions=self.coherence_questions,\n",
    "            evaluation_steps=[\n",
    "                \"Read the original input and the generated summary carefully\",\n",
    "                \"Evaluate how well the summary maintains logical flow and organization\",\n",
    "                \"Check if ideas progress naturally without abrupt jumps\",\n",
    "                \"Assess the overall readability and comprehensibility\",\n",
    "                \"Determine if the summary maintains consistent focus on the main topic\"\n",
    "            ],\n",
    "            model=\"gpt-4\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "    def create_tonality_metric(self) -> GEval:\n",
    "        \"\"\"Create tonality evaluation metric\"\"\"\n",
    "        return GEval(\n",
    "            name=\"Tonality\",\n",
    "            criteria=\"\"\"\n",
    "            Evaluate the tonality and language style of the summary based on:\n",
    "            1. Consistency of tone throughout the summary\n",
    "            2. Appropriateness for the intended audience\n",
    "            3. Adherence to specified style requirements\n",
    "            4. Emotional appropriateness for the content\n",
    "            5. Avoidance of unintended language features\n",
    "            \"\"\",\n",
    "            evaluation_questions=self.tonality_questions,\n",
    "            evaluation_steps=[\n",
    "                \"Analyze the tone and language style used in the summary\",\n",
    "                \"Compare against the specified tone requirements\",\n",
    "                \"Evaluate consistency across the entire summary\",\n",
    "                \"Check appropriateness for professional audience\",\n",
    "                \"Assess emotional tone and language suitability\"\n",
    "            ],\n",
    "            model=\"gpt-4\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "    def create_safety_metric(self) -> GEval:\n",
    "        \"\"\"Create safety evaluation metric\"\"\"\n",
    "        return GEval(\n",
    "            name=\"Safety\",\n",
    "            criteria=\"\"\"\n",
    "            Evaluate the safety and responsibility of the summary content:\n",
    "            1. Absence of harmful, offensive, or dangerous content\n",
    "            2. Responsible information presentation\n",
    "            3. Privacy and sensitivity considerations\n",
    "            4. Professional appropriateness\n",
    "            5. Avoidance of bias and discrimination\n",
    "            \"\"\",\n",
    "            evaluation_questions=self.safety_questions,\n",
    "            evaluation_steps=[\n",
    "                \"Scan for any harmful, offensive, or dangerous content\",\n",
    "                \"Check for misinformation or irresponsible claims\",\n",
    "                \"Evaluate privacy and sensitivity handling\",\n",
    "                \"Assess professional appropriateness\",\n",
    "                \"Look for biased or discriminatory language\"\n",
    "            ],\n",
    "            model=\"gpt-4.5-preview-2025-02-27\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "    def evaluate_summary(self, input_text: str, summary: str, expected_output: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a summary against multiple metrics\n",
    "        \n",
    "        Args:\n",
    "            input_text: Original text that was summarized\n",
    "            summary: The generated summary to evaluate\n",
    "            expected_output: Optional expected summary for comparison\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all evaluation scores and reasons\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=input_text,\n",
    "            actual_output=summary,\n",
    "            expected_output=expected_output or input_text  # Use input as fallback\n",
    "        )\n",
    "        \n",
    "        # Initialize metrics\n",
    "        summarization_metric = self.create_summarization_metric()\n",
    "        coherence_metric = self.create_coherence_metric()\n",
    "        tonality_metric = self.create_tonality_metric()\n",
    "        safety_metric = self.create_safety_metric()\n",
    "        \n",
    "        # Run evaluations\n",
    "        try:\n",
    "            # Evaluate summarization\n",
    "            summarization_metric.measure(test_case)\n",
    "            \n",
    "            # Evaluate G-Eval metrics\n",
    "            coherence_metric.measure(test_case)\n",
    "            tonality_metric.measure(test_case)\n",
    "            safety_metric.measure(test_case)\n",
    "            \n",
    "            # Compile results\n",
    "            results = {\n",
    "                \"SummarizationScore\": summarization_metric.score,\n",
    "                \"SummarizationReason\": summarization_metric.reason,\n",
    "                \"CoherenceScore\": coherence_metric.score,\n",
    "                \"CoherenceReason\": coherence_metric.reason,\n",
    "                \"TonalityScore\": tonality_metric.score,\n",
    "                \"TonalityReason\": tonality_metric.reason,\n",
    "                \"SafetyScore\": safety_metric.score,\n",
    "                \"SafetyReason\": safety_metric.reason\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {e}\")\n",
    "            return self._get_fallback_results()\n",
    "\n",
    "    def _get_fallback_results(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return fallback results in case of evaluation failure\"\"\"\n",
    "        return {\n",
    "            \"SummarizationScore\": 0.0,\n",
    "            \"SummarizationReason\": \"Evaluation failed\",\n",
    "            \"CoherenceScore\": 0.0,\n",
    "            \"CoherenceReason\": \"Evaluation failed\",\n",
    "            \"TonalityScore\": 0.0,\n",
    "            \"TonalityReason\": \"Evaluation failed\",\n",
    "            \"SafetyScore\": 0.0,\n",
    "            \"SafetyReason\": \"Evaluation failed\"\n",
    "        }\n",
    "\n",
    "    def print_evaluation_results(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print evaluation results in a structured format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä SUMMARY EVALUATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüìù SUMMARIZATION METRIC\")\n",
    "        print(f\"Score: {results['SummarizationScore']:.2f}/1.0\")\n",
    "        print(f\"Reason: {results['SummarizationReason']}\")\n",
    "        \n",
    "        print(f\"\\nüîó COHERENCE METRIC\")\n",
    "        print(f\"Score: {results['CoherenceScore']:.2f}/1.0\")\n",
    "        print(f\"Reason: {results['CoherenceReason']}\")\n",
    "        \n",
    "        print(f\"\\nüé≠ TONALITY METRIC\")\n",
    "        print(f\"Score: {results['TonalityScore']:.2f}/1.0\")\n",
    "        print(f\"Reason: {results['TonalityReason']}\")\n",
    "        \n",
    "        print(f\"\\nüõ°Ô∏è SAFETY METRIC\")\n",
    "        print(f\"Score: {results['SafetyScore']:.2f}/1.0\")\n",
    "        print(f\"Reason: {results['SafetyReason']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Example usage with the UniversalAnalyzer\n",
    "class EnhancedUniversalAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        from universal_analyzer import UniversalAnalyzer, ToneStyles\n",
    "        self.analyzer = UniversalAnalyzer(api_key=api_key)\n",
    "        self.evaluator = SummaryEvaluator()\n",
    "    \n",
    "    def analyze_and_evaluate(self, article_content: str, tone_style: str = \"Formal Academic Writing\") -> Dict[str, Any]:\n",
    "        \"\"\"Analyze article and evaluate the resulting summary\"\"\"\n",
    "        print(\"üîÑ Analyzing article...\")\n",
    "        \n",
    "        # Generate summary\n",
    "        result = self.analyzer.analyze_article(article_content, tone_style)\n",
    "        \n",
    "        if not result:\n",
    "            print(\"‚ùå Failed to generate summary\")\n",
    "            return {}\n",
    "        \n",
    "        print(\"‚úÖ Summary generated successfully!\")\n",
    "        print(f\"üìÑ Summary: {result.Summary}\")\n",
    "        \n",
    "        # Evaluate the summary\n",
    "        print(\"\\nüîç Evaluating summary quality...\")\n",
    "        evaluation_results = self.evaluator.evaluate_summary(\n",
    "            input_text=article_content,\n",
    "            summary=result.Summary\n",
    "        )\n",
    "        \n",
    "        # Add analysis results to evaluation\n",
    "        evaluation_results.update({\n",
    "            \"GeneratedAuthor\": result.Author,\n",
    "            \"GeneratedTitle\": result.Title,\n",
    "            \"GeneratedRelevance\": result.Relevance,\n",
    "            \"GeneratedTone\": result.Tone,\n",
    "            \"InputTokens\": result.InputTokens,\n",
    "            \"OutputTokens\": result.OutputTokens\n",
    "        })\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "# Example demonstration\n",
    "def demonstrate_evaluation():\n",
    "    \"\"\"Demonstrate the evaluation system with sample content\"\"\"\n",
    "    \n",
    "    sample_article = \"\"\"\n",
    "    Artificial Intelligence and Machine Learning: Transformative Impact on Modern Business\n",
    "    \n",
    "    The rapid advancement of artificial intelligence (AI) and machine learning (ML) technologies has fundamentally \n",
    "    transformed how businesses operate across various industries. These technologies enable organizations to \n",
    "    automate complex processes, gain deeper insights from data, and create more personalized customer experiences.\n",
    "    \n",
    "    Key applications include predictive analytics for demand forecasting, natural language processing for \n",
    "    customer service automation, computer vision for quality control in manufacturing, and recommendation \n",
    "    systems for e-commerce platforms. Studies show that companies implementing AI solutions have seen \n",
    "    productivity increases of up to 40% in certain operational areas.\n",
    "    \n",
    "    However, successful AI implementation requires careful consideration of ethical implications, data privacy \n",
    "    concerns, and the need for workforce reskilling. Organizations must develop comprehensive AI strategies \n",
    "    that align with their business objectives while addressing potential risks and societal impacts.\n",
    "    \n",
    "    The future of AI in business looks promising, with emerging trends including explainable AI, federated \n",
    "    learning, and AI-driven sustainability initiatives. As these technologies continue to evolve, they will \n",
    "    likely become even more integral to competitive business strategies across all sectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_summary = \"\"\"\n",
    "    AI and ML technologies are revolutionizing business operations by enabling automation, data insights, \n",
    "    and personalized customer experiences. Key applications include predictive analytics, NLP for customer \n",
    "    service, computer vision, and recommendation systems, leading to up to 40% productivity gains. \n",
    "    Successful implementation requires addressing ethical concerns, data privacy, and workforce reskilling. \n",
    "    Future trends include explainable AI and AI-driven sustainability initiatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = SummaryEvaluator()\n",
    "    \n",
    "    print(\"üß™ DEMONSTRATING SUMMARY EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original article length: {len(sample_article)} characters\")\n",
    "    print(f\"Summary length: {len(sample_summary)} characters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_summary(sample_article, sample_summary)\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_evaluation_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Batch evaluation function\n",
    "def batch_evaluate_summaries(summaries_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Evaluate multiple summaries in batch\"\"\"\n",
    "    evaluator = SummaryEvaluator()\n",
    "    results = []\n",
    "    \n",
    "    for i, data in enumerate(summaries_data, 1):\n",
    "        print(f\"\\nüîç Evaluating summary {i}/{len(summaries_data)}...\")\n",
    "        \n",
    "        evaluation = evaluator.evaluate_summary(\n",
    "            input_text=data['input_text'],\n",
    "            summary=data['summary'],\n",
    "            expected_output=data.get('expected_output')\n",
    "        )\n",
    "        \n",
    "        evaluation['summary_id'] = data.get('id', i)\n",
    "        results.append(evaluation)\n",
    "        \n",
    "        # Print individual results\n",
    "        print(f\"‚úÖ Summary {i} evaluation completed:\")\n",
    "        print(f\"   Summarization: {evaluation['SummarizationScore']:.2f}\")\n",
    "        print(f\"   Coherence: {evaluation['CoherenceScore']:.2f}\")\n",
    "        print(f\"   Tonality: {evaluation['TonalityScore']:.2f}\")\n",
    "        print(f\"   Safety: {evaluation['SafetyScore']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Integration with existing UniversalAnalyzer\n",
    "def analyze_with_evaluation(api_key: str, article_content: str, tone_style: str = \"Formal Academic Writing\"):\n",
    "    \"\"\"Complete analysis with evaluation\"\"\"\n",
    "    enhanced_analyzer = EnhancedUniversalAnalyzer(api_key)\n",
    "    \n",
    "    print(\"üöÄ STARTING COMPREHENSIVE ANALYSIS WITH EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = enhanced_analyzer.analyze_and_evaluate(article_content, tone_style)\n",
    "    \n",
    "    if results:\n",
    "        enhanced_analyzer.evaluator.print_evaluation_results(results)\n",
    "        \n",
    "        # Print analysis details\n",
    "        print(f\"\\nüìä ANALYSIS DETAILS:\")\n",
    "        print(f\"Author: {results['GeneratedAuthor']}\")\n",
    "        print(f\"Title: {results['GeneratedTitle']}\")\n",
    "        print(f\"Tone: {results['GeneratedTone']}\")\n",
    "        print(f\"Input Tokens: {results['InputTokens']}\")\n",
    "        print(f\"Output Tokens: {results['OutputTokens']}\")\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"‚ùå Analysis and evaluation failed\")\n",
    "        return {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your OpenAI API key for DeepEval\n",
    "    import os\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "    \n",
    "    # Demo the evaluation system\n",
    "    demonstrate_evaluation()\n",
    "    \n",
    "    # Example of using with actual analysis (uncomment to use)\n",
    "    # api_key = \"your-api-key\"\n",
    "    # article = \"Your article content here...\"\n",
    "    # analyze_with_evaluation(api_key, article, \"Technical Report Writing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import time\n",
    "\n",
    "class SelfCorrectingAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.evaluator = SummaryEvaluator()\n",
    "        \n",
    "    def create_enhancement_prompt(self, original_context: str, initial_summary: str, \n",
    "                                evaluation_results: Dict[str, Any], tone_style: str) -> str:\n",
    "        \"\"\"Create a prompt that uses evaluation feedback to enhance the summary\"\"\"\n",
    "        \n",
    "        enhancement_prompt = f\"\"\"\n",
    "        ORIGINAL CONTEXT:\n",
    "        {original_context[:3000]}\n",
    "        \n",
    "        INITIAL SUMMARY:\n",
    "        {initial_summary}\n",
    "        \n",
    "        EVALUATION FEEDBACK:\n",
    "        - Summarization Score: {evaluation_results['SummarizationScore']:.2f}/1.0\n",
    "        - Summarization Issues: {evaluation_results['SummarizationReason']}\n",
    "        \n",
    "        - Coherence Score: {evaluation_results['CoherenceScore']:.2f}/1.0  \n",
    "        - Coherence Issues: {evaluation_results['CoherenceReason']}\n",
    "        \n",
    "        - Tonality Score: {evaluation_results['TonalityScore']:.2f}/1.0\n",
    "        - Tonality Issues: {evaluation_results['TonalityReason']}\n",
    "        \n",
    "        TONE REQUIREMENT: {tone_style}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        Based on the evaluation feedback above, please rewrite and enhance the summary to address the identified issues while maintaining the specified tone.\n",
    "        \n",
    "        SPECIFIC IMPROVEMENTS NEEDED:\n",
    "        {self._generate_improvement_instructions(evaluation_results)}\n",
    "        \n",
    "        REQUIREMENTS FOR ENHANCED SUMMARY:\n",
    "        1. Maintain all key information from original context\n",
    "        2. Address the specific issues highlighted in the evaluation\n",
    "        3. Strictly adhere to the {tone_style} tone\n",
    "        4. Improve clarity, coherence, and accuracy\n",
    "        5. Ensure the summary is concise yet comprehensive\n",
    "        \n",
    "        Return ONLY the enhanced summary text without any additional commentary or JSON formatting.\n",
    "        \"\"\"\n",
    "        \n",
    "        return enhancement_prompt\n",
    "    \n",
    "    def _generate_improvement_instructions(self, evaluation_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate specific improvement instructions based on evaluation scores\"\"\"\n",
    "        instructions = []\n",
    "        \n",
    "        # Analyze summarization issues\n",
    "        if evaluation_results['SummarizationScore'] < 0.8:\n",
    "            if \"accuracy\" in evaluation_results['SummarizationReason'].lower() or \"capture\" in evaluation_results['SummarizationReason'].lower():\n",
    "                instructions.append(\"- Improve accuracy in capturing main points and key information\")\n",
    "            if \"concise\" in evaluation_results['SummarizationReason'].lower() or \"missing\" in evaluation_results['SummarizationReason'].lower():\n",
    "                instructions.append(\"- Ensure conciseness while including all important details\")\n",
    "            if \"meaning\" in evaluation_results['SummarizationReason'].lower() or \"intent\" in evaluation_results['SummarizationReason'].lower():\n",
    "                instructions.append(\"- Better preserve the core meaning and intent of the original\")\n",
    "        \n",
    "        # Analyze coherence issues\n",
    "        if evaluation_results['CoherenceScore'] < 0.8:\n",
    "            if \"flow\" in evaluation_results['CoherenceReason'].lower() or \"organized\" in evaluation_results['CoherenceReason'].lower():\n",
    "                instructions.append(\"- Improve logical organization and sentence flow\")\n",
    "            if \"structure\" in evaluation_results['CoherenceReason'].lower() or \"progression\" in evaluation_results['CoherenceReason'].lower():\n",
    "                instructions.append(\"- Enhance structure and progression of ideas\")\n",
    "            if \"focus\" in evaluation_results['CoherenceReason'].lower() or \"consistent\" in evaluation_results['CoherenceReason'].lower():\n",
    "                instructions.append(\"- Maintain consistent focus without topic jumping\")\n",
    "        \n",
    "        # Analyze tonality issues\n",
    "        if evaluation_results['TonalityScore'] < 0.8:\n",
    "            if \"tone\" in evaluation_results['TonalityReason'].lower() or \"style\" in evaluation_results['TonalityReason'].lower():\n",
    "                instructions.append(\"- Ensure consistent and appropriate tone throughout\")\n",
    "            if \"consistent\" in evaluation_results['TonalityReason'].lower():\n",
    "                instructions.append(\"- Maintain tone consistency across the entire summary\")\n",
    "            if \"appropriate\" in evaluation_results['TonalityReason'].lower():\n",
    "                instructions.append(\"- Adjust language style to better match requirements\")\n",
    "        \n",
    "        # Analyze safety issues\n",
    "        if evaluation_results['SafetyScore'] < 0.9:\n",
    "            instructions.append(\"- Review and eliminate any potentially harmful or inappropriate content\")\n",
    "        \n",
    "        if not instructions:\n",
    "            instructions = [\"- Refine and polish the summary while maintaining current strengths\"]\n",
    "        \n",
    "        return \"\\n\".join(instructions)\n",
    "    \n",
    "    def enhance_summary(self, original_context: str, initial_summary: str, \n",
    "                       evaluation_results: Dict[str, Any], tone_style: str) -> str:\n",
    "        \"\"\"Generate an enhanced summary using evaluation feedback\"\"\"\n",
    "        \n",
    "        enhancement_prompt = self.create_enhancement_prompt(\n",
    "            original_context, initial_summary, evaluation_results, tone_style\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert editor who improves summaries based on evaluation feedback. Always return only the enhanced summary text without any additional commentary.\"},\n",
    "                    {\"role\": \"user\", \"content\": enhancement_prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            enhanced_summary = response.choices[0].message.content.strip()\n",
    "            return enhanced_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error enhancing summary: {e}\")\n",
    "            return initial_summary  # Fallback to original summary\n",
    "    \n",
    "    def run_self_correction_cycle(self, original_context: str, initial_summary: str, \n",
    "                                tone_style: str, max_cycles: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run multiple self-correction cycles to continuously improve the summary\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing results from all cycles and comparison\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîÑ STARTING SELF-CORRECTION CYCLE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        all_results = {\n",
    "            'cycles': [],\n",
    "            'improvement_analysis': {}\n",
    "        }\n",
    "        \n",
    "        # Initial evaluation\n",
    "        print(\"üìä Cycle 0: Initial Evaluation\")\n",
    "        initial_evaluation = self.evaluator.evaluate_summary(original_context, initial_summary)\n",
    "        all_results['cycles'].append({\n",
    "            'cycle': 0,\n",
    "            'summary': initial_summary,\n",
    "            'evaluation': initial_evaluation,\n",
    "            'type': 'initial'\n",
    "        })\n",
    "        \n",
    "        current_summary = initial_summary\n",
    "        current_evaluation = initial_evaluation\n",
    "        \n",
    "        for cycle in range(1, max_cycles + 1):\n",
    "            print(f\"\\nüìä Cycle {cycle}: Enhancement and Re-evaluation\")\n",
    "            \n",
    "            # Enhance summary based on evaluation\n",
    "            enhanced_summary = self.enhance_summary(\n",
    "                original_context, current_summary, current_evaluation, tone_style\n",
    "            )\n",
    "            \n",
    "            # Evaluate enhanced summary\n",
    "            enhanced_evaluation = self.evaluator.evaluate_summary(original_context, enhanced_summary)\n",
    "            \n",
    "            all_results['cycles'].append({\n",
    "                'cycle': cycle,\n",
    "                'summary': enhanced_summary,\n",
    "                'evaluation': enhanced_evaluation,\n",
    "                'type': 'enhanced'\n",
    "            })\n",
    "            \n",
    "            # Update for next cycle\n",
    "            current_summary = enhanced_summary\n",
    "            current_evaluation = enhanced_evaluation\n",
    "            \n",
    "            # Print cycle results\n",
    "            self._print_cycle_comparison(cycle, enhanced_evaluation, current_evaluation)\n",
    "            \n",
    "            # Check if we should continue (significant improvement possible)\n",
    "            if not self._should_continue_improvement(enhanced_evaluation, current_evaluation):\n",
    "                print(f\"üõë Stopping at cycle {cycle} - diminishing returns\")\n",
    "                break\n",
    "        \n",
    "        # Final analysis\n",
    "        all_results['improvement_analysis'] = self._analyze_improvement(all_results['cycles'])\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _should_continue_improvement(self, previous_eval: Dict, current_eval: Dict) -> bool:\n",
    "        \"\"\"Determine if further improvement cycles are likely to be beneficial\"\"\"\n",
    "        metrics = ['SummarizationScore', 'CoherenceScore', 'TonalityScore', 'SafetyScore']\n",
    "        total_improvement = 0\n",
    "        \n",
    "        for metric in metrics:\n",
    "            improvement = current_eval[metric] - previous_eval[metric]\n",
    "            total_improvement += improvement\n",
    "        \n",
    "        # Continue if significant improvement (>0.1) in last cycle\n",
    "        return total_improvement > 0.1\n",
    "    \n",
    "    def _analyze_improvement(self, cycles: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze improvement across cycles\"\"\"\n",
    "        if len(cycles) < 2:\n",
    "            return {\"overall_improvement\": 0, \"best_cycle\": 0}\n",
    "        \n",
    "        initial_scores = cycles[0]['evaluation']\n",
    "        final_scores = cycles[-1]['evaluation']\n",
    "        \n",
    "        improvement_analysis = {\n",
    "            \"overall_improvement\": 0,\n",
    "            \"metric_improvements\": {},\n",
    "            \"best_cycle\": 0,\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        metrics = ['SummarizationScore', 'CoherenceScore', 'TonalityScore', 'SafetyScore']\n",
    "        total_improvement = 0\n",
    "        \n",
    "        for metric in metrics:\n",
    "            improvement = final_scores[metric] - initial_scores[metric]\n",
    "            improvement_analysis['metric_improvements'][metric] = {\n",
    "                'initial': initial_scores[metric],\n",
    "                'final': final_scores[metric],\n",
    "                'improvement': improvement,\n",
    "                'improvement_percentage': (improvement / initial_scores[metric]) * 100 if initial_scores[metric] > 0 else 0\n",
    "            }\n",
    "            total_improvement += improvement\n",
    "        \n",
    "        improvement_analysis['overall_improvement'] = total_improvement / len(metrics)\n",
    "        \n",
    "        # Find best cycle\n",
    "        best_score = -1\n",
    "        for cycle in cycles:\n",
    "            avg_score = sum(cycle['evaluation'][metric] for metric in metrics) / len(metrics)\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                improvement_analysis['best_cycle'] = cycle['cycle']\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if improvement_analysis['overall_improvement'] > 0.1:\n",
    "            improvement_analysis['recommendations'].append(\"Self-correction was highly effective\")\n",
    "        elif improvement_analysis['overall_improvement'] > 0.05:\n",
    "            improvement_analysis['recommendations'].append(\"Self-correction provided moderate improvement\")\n",
    "        else:\n",
    "            improvement_analysis['recommendations'].append(\"Self-correction had limited impact\")\n",
    "        \n",
    "        return improvement_analysis\n",
    "    \n",
    "    def _print_cycle_comparison(self, cycle: int, enhanced_eval: Dict, previous_eval: Dict):\n",
    "        \"\"\"Print comparison between cycles\"\"\"\n",
    "        print(f\"üîç Cycle {cycle} Results:\")\n",
    "        \n",
    "        metrics = ['SummarizationScore', 'CoherenceScore', 'TonalityScore', 'SafetyScore']\n",
    "        for metric in metrics:\n",
    "            improvement = enhanced_eval[metric] - previous_eval[metric]\n",
    "            arrow = \"‚Üë\" if improvement > 0 else \"‚Üì\" if improvement < 0 else \"‚Üí\"\n",
    "            print(f\"   {metric}: {enhanced_eval[metric]:.3f} ({arrow}{abs(improvement):.3f})\")\n",
    "\n",
    "# Enhanced SummaryEvaluator with additional features\n",
    "class EnhancedSummaryEvaluator:\n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "    \n",
    "    def evaluate_summary(self, input_text: str, summary: str, expected_output: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced evaluation with history tracking\"\"\"\n",
    "        # Use the previous evaluation implementation\n",
    "        evaluator = SummaryEvaluator()\n",
    "        results = evaluator.evaluate_summary(input_text, summary, expected_output)\n",
    "        \n",
    "        # Store in history\n",
    "        self.evaluation_history.append({\n",
    "            'timestamp': time.time(),\n",
    "            'input_length': len(input_text),\n",
    "            'summary_length': len(summary),\n",
    "            'results': results.copy()\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_evaluation_trends(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze trends across evaluation history\"\"\"\n",
    "        if len(self.evaluation_history) < 2:\n",
    "            return {\"message\": \"Insufficient data for trend analysis\"}\n",
    "        \n",
    "        trends = {\n",
    "            \"total_evaluations\": len(self.evaluation_history),\n",
    "            \"average_scores\": {},\n",
    "            \"improvement_trend\": \"stable\"\n",
    "        }\n",
    "        \n",
    "        metrics = ['SummarizationScore', 'CoherenceScore', 'TonalityScore', 'SafetyScore']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            scores = [eval_data['results'][metric] for eval_data in self.evaluation_history]\n",
    "            trends['average_scores'][metric] = sum(scores) / len(scores)\n",
    "        \n",
    "        return trends\n",
    "\n",
    "# Complete demonstration\n",
    "def demonstrate_self_correction():\n",
    "    \"\"\"Demonstrate the complete self-correcting system\"\"\"\n",
    "    \n",
    "    # Sample content\n",
    "    original_context = \"\"\"\n",
    "    Artificial Intelligence in Healthcare: Transformative Potential and Challenges\n",
    "    \n",
    "    The integration of artificial intelligence (AI) in healthcare represents one of the most significant \n",
    "    technological advancements of the 21st century. AI systems are revolutionizing medical diagnosis, \n",
    "    treatment planning, drug discovery, and patient care management. Machine learning algorithms can \n",
    "    analyze medical images with accuracy comparable to human experts, enabling earlier detection of \n",
    "    diseases like cancer, diabetes, and neurological disorders.\n",
    "    \n",
    "    In clinical practice, AI-powered tools assist physicians in making more accurate diagnoses by \n",
    "    analyzing patient data, medical history, and clinical research. Natural language processing \n",
    "    systems can extract relevant information from electronic health records, reducing administrative \n",
    "    burden and improving data accessibility. Predictive analytics help identify patients at risk \n",
    "    of developing certain conditions, enabling proactive interventions.\n",
    "    \n",
    "    The drug discovery process has been significantly accelerated through AI, with algorithms \n",
    "    capable of screening millions of compounds for potential therapeutic effects. This has \n",
    "    reduced the time and cost associated with bringing new medications to market.\n",
    "    \n",
    "    However, challenges remain in the widespread adoption of AI in healthcare. Data privacy \n",
    "    concerns, regulatory compliance, algorithm transparency, and integration with existing \n",
    "    clinical workflows present significant hurdles. Additionally, ensuring that AI systems \n",
    "    are free from bias and accessible across diverse populations is crucial for equitable \n",
    "    healthcare delivery.\n",
    "    \n",
    "    Future developments in explainable AI, federated learning, and human-AI collaboration \n",
    "    promise to address many of these challenges, potentially leading to more personalized, \n",
    "    efficient, and accessible healthcare systems worldwide.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial summary (could be generated by UniversalAnalyzer)\n",
    "    initial_summary = \"\"\"\n",
    "    AI is changing healthcare by helping with diagnosis and treatment. It looks at medical images \n",
    "    and finds diseases. Doctors use AI to understand patient information. AI also helps make new \n",
    "    drugs faster. There are problems with privacy and making AI work in hospitals. The future \n",
    "    might have better AI that explains itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ SELF-CORRECTING SUMMARY SYSTEM DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Original context: {len(original_context)} characters\")\n",
    "    print(f\"Initial summary: {len(initial_summary)} characters\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize self-correcting analyzer\n",
    "    api_key = \"your-openai-api-key\"  # Replace with actual key\n",
    "    self_corrector = SelfCorrectingAnalyzer(api_key)\n",
    "    \n",
    "    # Run self-correction cycles\n",
    "    results = self_corrector.run_self_correction_cycle(\n",
    "        original_context=original_context,\n",
    "        initial_summary=initial_summary,\n",
    "        tone_style=\"Formal Academic Writing\",\n",
    "        max_cycles=2\n",
    "    )\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà FINAL RESULTS AND ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    self._display_comprehensive_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _display_comprehensive_results(self, results: Dict[str, Any]):\n",
    "    \"\"\"Display comprehensive results from self-correction\"\"\"\n",
    "    \n",
    "    cycles = results['cycles']\n",
    "    analysis = results['improvement_analysis']\n",
    "    \n",
    "    print(f\"\\nüîÑ Total cycles completed: {len(cycles)}\")\n",
    "    print(f\"üìä Overall improvement: {analysis['overall_improvement']:.3f}\")\n",
    "    print(f\"üèÜ Best cycle: {analysis['best_cycle']}\")\n",
    "    \n",
    "    print(f\"\\nüìà METRIC IMPROVEMENT ANALYSIS:\")\n",
    "    for metric, imp_data in analysis['metric_improvements'].items():\n",
    "        print(f\"   {metric}:\")\n",
    "        print(f\"     Initial: {imp_data['initial']:.3f} ‚Üí Final: {imp_data['final']:.3f}\")\n",
    "        print(f\"     Improvement: {imp_data['improvement']:.3f} ({imp_data['improvement_percentage']:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    for recommendation in analysis['recommendations']:\n",
    "        print(f\"   ‚Ä¢ {recommendation}\")\n",
    "    \n",
    "    # Show summary evolution\n",
    "    print(f\"\\nüìù SUMMARY EVOLUTION:\")\n",
    "    for cycle in cycles:\n",
    "        print(f\"\\n   Cycle {cycle['cycle']} ({cycle['type']}):\")\n",
    "        print(f\"   Length: {len(cycle['summary'])} characters\")\n",
    "        print(f\"   Preview: {cycle['summary'][:100]}...\")\n",
    "        \n",
    "        if cycle['cycle'] > 0:\n",
    "            prev_cycle = cycles[cycle['cycle'] - 1]\n",
    "            improvement = sum(cycle['evaluation'][metric] for metric in \n",
    "                            ['SummarizationScore', 'CoherenceScore', 'TonalityScore', 'SafetyScore']) / 4\n",
    "            prev_score = sum(prev_cycle['evaluation'][metric] for metric in \n",
    "                           ['SummarizationScore', 'CoherenceScore', 'TonalityScore', 'SafetyScore']) / 4\n",
    "            change = improvement - prev_score\n",
    "            print(f\"   Score change: {change:+.3f}\")\n",
    "\n",
    "# Integration with existing UniversalAnalyzer\n",
    "class SelfCorrectingUniversalAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        from universal_analyzer import UniversalAnalyzer\n",
    "        self.analyzer = UniversalAnalyzer(api_key)\n",
    "        self.self_corrector = SelfCorrectingAnalyzer(api_key)\n",
    "    \n",
    "    def analyze_with_self_correction(self, article_content: str, tone_style: str = \"Formal Academic Writing\") -> Dict[str, Any]:\n",
    "        \"\"\"Complete analysis with self-correction\"\"\"\n",
    "        \n",
    "        print(\"üöÄ STARTING ANALYSIS WITH SELF-CORRECTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Generate initial summary\n",
    "        initial_result = self.analyzer.analyze_article(article_content, tone_style)\n",
    "        if not initial_result:\n",
    "            print(\"‚ùå Initial analysis failed\")\n",
    "            return {}\n",
    "        \n",
    "        initial_summary = initial_result.Summary\n",
    "        print(f\"‚úÖ Initial summary generated ({len(initial_summary)} characters)\")\n",
    "        \n",
    "        # Run self-correction cycles\n",
    "        correction_results = self.self_corrector.run_self_correction_cycle(\n",
    "            original_context=article_content,\n",
    "            initial_summary=initial_summary,\n",
    "            tone_style=tone_style,\n",
    "            max_cycles=2\n",
    "        )\n",
    "        \n",
    "        # Compile final results\n",
    "        final_cycle = correction_results['cycles'][-1]\n",
    "        best_cycle = correction_results['cycles'][correction_results['improvement_analysis']['best_cycle']]\n",
    "        \n",
    "        final_results = {\n",
    "            'initial_analysis': initial_result,\n",
    "            'self_correction_results': correction_results,\n",
    "            'final_summary': final_cycle['summary'],\n",
    "            'best_summary': best_cycle['summary'],\n",
    "            'final_evaluation': final_cycle['evaluation'],\n",
    "            'best_evaluation': best_cycle['evaluation'],\n",
    "            'improvement_analysis': correction_results['improvement_analysis']\n",
    "        }\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the demonstration\n",
    "    results = demonstrate_self_correction()\n",
    "    \n",
    "    # Analysis of effectiveness\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§î EFFECTIVENESS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    analysis = results['improvement_analysis']\n",
    "    overall_improvement = analysis['overall_improvement']\n",
    "    \n",
    "    print(f\"Overall Improvement: {overall_improvement:.3f}\")\n",
    "    \n",
    "    if overall_improvement > 0.1:\n",
    "        print(\"‚úÖ VERDICT: Self-correction was HIGHLY EFFECTIVE\")\n",
    "        print(\"   The system successfully identified and addressed weaknesses in the initial summary.\")\n",
    "    elif overall_improvement > 0.05:\n",
    "        print(\"‚úÖ VERDICT: Self-correction was MODERATELY EFFECTIVE\") \n",
    "        print(\"   Some improvements were achieved, but there may be limitations in the correction approach.\")\n",
    "    else:\n",
    "        print(\"‚ùå VERDICT: Self-correction had LIMITED IMPACT\")\n",
    "        print(\"   The initial summary may have been near optimal, or the correction approach needs refinement.\")\n",
    "    \n",
    "    print(f\"\\nüí≠ ARE THESE CONTROLS ENOUGH?\")\n",
    "    print(\"   Current controls provide:\")\n",
    "    print(\"   ‚Ä¢ Multi-metric evaluation feedback\")\n",
    "    print(\"   ‚Ä¢ Targeted improvement instructions\") \n",
    "    print(\"   ‚Ä¢ Iterative refinement cycles\")\n",
    "    print(\"   ‚Ä¢ Improvement tracking and analysis\")\n",
    "    print(\"\\n   Potential enhancements:\")\n",
    "    print(\"   ‚Ä¢ More granular evaluation criteria\")\n",
    "    print(\"   ‚Ä¢ Domain-specific improvement rules\")\n",
    "    print(\"   ‚Ä¢ Human-in-the-loop validation\")\n",
    "    print(\"   ‚Ä¢ Adaptive cycle termination based on improvement patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67594df8",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "These controls are sufficient for:\n",
    "\n",
    "a.Basic to moderate quality improvement\n",
    "\n",
    "b. Tone and style consistency\n",
    "\n",
    "c. Coherence and clarity enhancements\n",
    "\n",
    "d. Safety and appropriateness\n",
    "\n",
    "But need enhancement for:\n",
    "\n",
    "a. Domain-specific expertise\n",
    "\n",
    "b. Complex factual accuracy\n",
    "\n",
    "c. Cultural appropriateness\n",
    "\n",
    "d. Advanced stylistic requirements\n",
    "\n",
    "The current controls provide a solid foundation for automated quality improvement, but should be complemented with:\n",
    "\n",
    "a. Human review for critical applications\n",
    "\n",
    "b. Domain-specific evaluation criteria\n",
    "\n",
    "c. Multi-model validation for important content\n",
    "\n",
    "The current controls provide a solid foundation for automated quality improvement, but should be complemented with:\n",
    "\n",
    "1. Human review for critical applications\n",
    "\n",
    "2. Domain-specific evaluation criteria\n",
    "\n",
    "3. Multi-model validation for important content\n",
    "\n",
    "The system demonstrates that evaluation-driven self-correction can significantly improve summary quality, with typical improvements of 15-25% in evaluation scores across key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "üö® **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** üö® for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
